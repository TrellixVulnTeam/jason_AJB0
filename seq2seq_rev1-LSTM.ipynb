{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sequence to sequence example in Keras (character-level).\\n\\nThis script demonstrates how to implement a basic character-level\\nsequence-to-sequence model. We apply it to translating\\nshort English sentences into short French sentences,\\ncharacter-by-character. Note that it is fairly unusual to\\ndo character-level machine translation, as word-level\\nmodels are more common in this domain.\\n\\n# Summary of the algorithm\\n\\n- We start with input sequences from a domain (e.g. English sentences)\\n    and correspding target sequences from another domain\\n    (e.g. French sentences).\\n- An encoder LSTM turns input sequences to 2 state vectors\\n    (we keep the last LSTM state and discard the outputs).\\n- A decoder LSTM is trained to turn the target sequences into\\n    the same sequence but offset by one timestep in the future,\\n    a training process called \"teacher forcing\" in this context.\\n    Is uses as initial state the state vectors from the encoder.\\n    Effectively, the decoder learns to generate `targets[t+1...]`\\n    given `targets[...t]`, conditioned on the input sequence.\\n- In inference mode, when we want to decode unknown input sequences, we:\\n    - Encode the input sequence into state vectors\\n    - Start with a target sequence of size 1\\n        (just the start-of-sequence character)\\n    - Feed the state vectors and 1-char target sequence\\n        to the decoder to produce predictions for the next character\\n    - Sample the next character using these predictions\\n        (we simply use argmax).\\n    - Append the sampled character to the target sequence\\n    - Repeat until we generate the end-of-sequence character or we\\n        hit the character limit.\\n\\n# Data download\\n\\nEnglish to French sentence pairs.\\nhttp://www.manythings.org/anki/fra-eng.zip\\n\\nLots of neat sentence pairs datasets can be found at:\\nhttp://www.manythings.org/anki/\\n\\n# References\\n\\n- Sequence to Sequence Learning with Neural Networks\\n    https://arxiv.org/abs/1409.3215\\n- Learning Phrase Representations using\\n    RNN Encoder-Decoder for Statistical Machine Translation\\n    https://arxiv.org/abs/1406.1078\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Sequence to sequence example in Keras (character-level).\n",
    "\n",
    "This script demonstrates how to implement a basic character-level\n",
    "sequence-to-sequence model. We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "\n",
    "# Summary of the algorithm\n",
    "\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and correspding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    Is uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "\n",
    "# Data download\n",
    "\n",
    "English to French sentence pairs.\n",
    "http://www.manythings.org/anki/fra-eng.zip\n",
    "\n",
    "Lots of neat sentence pairs datasets can be found at:\n",
    "http://www.manythings.org/anki/\n",
    "\n",
    "# References\n",
    "\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.1078\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples =10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'data5000delspace.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 4999\n",
      "Number of unique input tokens: 3132\n",
      "Number of unique output tokens: 3010\n",
      "Max sequence length for inputs: 53\n",
      "Max sequence length for outputs: 57\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exists\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from keras.models import load_model\n",
    "model_name = 's2slstm.h5'\n",
    "\n",
    "if os.path.isfile(model_name):\n",
    "    print('file exists')\n",
    "    #model = load_model(model_name)\n",
    "    \n",
    "    # layers[0...4]= 0:encoder_input, 1:decoder_input, 2:encoder_lstm, 3:decoder_lstm, 4:decoder_output\n",
    "    #encoder_inputs = model.layers[0]\n",
    "    #encoder = model.layers[2]\n",
    "    #decoder_lstm = model.layers[3]\n",
    "    #decoder_dense = model.layers[4]\n",
    "    \n",
    "    #encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    #encoder_states = [state_h, state_c]\n",
    "    \n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens),name='encoder_input')\n",
    "    encoder = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens),name='decoder_input')\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_output')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.load_weights(model_name)\n",
    "\n",
    "else:\n",
    "    print('file not exists')\n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens),name='encoder_input')\n",
    "    encoder = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens),name='decoder_input')\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the\n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_output')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    # Define the model that will turn\n",
    "    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    # Run training\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "    # Save model\n",
    "    model.save_weights(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.core.Dense object at 0x000001FE190E2EB8>\n"
     ]
    }
   ],
   "source": [
    "print(decoder_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trainable': True, 'activity_regularizer': None, 'supports_masking': True, 'stateful': False, 'name': 'decoder_output', 'bias_constraint': None, 'kernel_constraint': None, '_built': True, '_outbound_nodes': [], '_non_trainable_weights': [], '_initial_weights': None, '_per_input_losses': {}, 'use_bias': True, 'bias_initializer': <keras.initializers.Zeros object at 0x000001FE190FF208>, 'input_spec': InputSpec(min_ndim=2, axes={-1: 256}), '_trainable_weights': [<tf.Variable 'decoder_output/kernel:0' shape=(256, 3010) dtype=float32_ref>, <tf.Variable 'decoder_output/bias:0' shape=(3010,) dtype=float32_ref>], 'bias': <tf.Variable 'decoder_output/bias:0' shape=(3010,) dtype=float32_ref>, 'kernel': <tf.Variable 'decoder_output/kernel:0' shape=(256, 3010) dtype=float32_ref>, 'kernel_regularizer': None, 'activation': <function softmax at 0x000001FBB9E48E18>, '_per_input_updates': {}, '_inbound_nodes': [<keras.engine.base_layer.Node object at 0x000001FE1A2A8CC0>], '_updates': [], 'kernel_initializer': <keras.initializers.VarianceScaling object at 0x000001FE190FF128>, '_losses': [], 'units': 3010, 'bias_regularizer': None}\n"
     ]
    }
   ],
   "source": [
    "print(decoder_dense.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 中国移动营销行来发展报告alink\n",
      "Decoded sentence: 去过那么多次，从来没见过\n",
      "\n",
      "-\n",
      "Input sentence: 小马也疯狂------地位之争。\n",
      "Decoded sentence: 就是一天，又一个人，我无聊得\n",
      "\n",
      "-\n",
      "Input sentence: 那些年，我们一起偷看过的电视。「暴走漫画」\n",
      "Decoded sentence: 真不愧是这么走出来的爹········\n",
      "\n",
      "-\n",
      "Input sentence: 北京的小纯洁们，周日见。#硬汉摆拍清纯照#\n",
      "Decoded sentence: 看到的孩子堆：条里是最女人的\n",
      "\n",
      "-\n",
      "Input sentence: 要是这一年哭泣的理由不再是难过而是感动会多么好\n",
      "Decoded sentence: 我已经快感动得哭了。\n",
      "\n",
      "-\n",
      "Input sentence: 对于国内动漫画作者引用工笔素材的一些个人意见。\n",
      "Decoded sentence: 人生的是多少，钱的人出\n",
      "\n",
      "-\n",
      "Input sentence: 猫咪保镖最赞了！你们看懂了吗？！（来自：9gag）\n",
      "Decoded sentence: 要求反雷就是要不cwicne··\n",
      "\n",
      "-\n",
      "Input sentence: 莫愁和所有人开了一个玩笑——其实，她是会正常唱歌的……\n",
      "Decoded sentence: 是，我的天告我小我，老却就下了回床。\n",
      "\n",
      "-\n",
      "Input sentence: 你见过皮卡丘喝水的样子吗？\n",
      "Decoded sentence: 小互你的粉丝肯定大\n",
      "\n",
      "-\n",
      "Input sentence: 如果有个人能让你忘掉过去，那TA很可能就是你的未来。\n",
      "Decoded sentence: 下面的事儿们有必要多么多大王会！\n",
      "\n",
      "-\n",
      "Input sentence: 我在北京，24岁，想去马尔代夫，一个人。\n",
      "Decoded sentence: 这太可以了，哪里没有高呢？\n",
      "\n",
      "-\n",
      "Input sentence: 哥你还跳不跳楼了？我们要下班啊！\n",
      "Decoded sentence: 我想去很多，钱就是我。\n",
      "\n",
      "-\n",
      "Input sentence: 龙生龙，凤生凤，是个喵咪它就萌。\n",
      "Decoded sentence: 老爷子不会长大，你心你呢？\n",
      "\n",
      "-\n",
      "Input sentence: 从胚胎期开始的面部特征演变过程\n",
      "Decoded sentence: 真心佩服凯子的精神！\n",
      "\n",
      "-\n",
      "Input sentence: 本届轮值主席王石致开幕词。讲60岁上哈佛。\n",
      "Decoded sentence: 是啊，你，这么线我，幸福我生日在中\n",
      "\n",
      "-\n",
      "Input sentence: 非常不喜欢北京现在的天气……非常……\n",
      "Decoded sentence: 我也不喜欢那种又冷又下雨的天气了！\n",
      "\n",
      "-\n",
      "Input sentence: 我第一次坐飞机是进安达信的入职培训，在深圳。你们哪？\n",
      "Decoded sentence: 哈哈，这是怎么样的能越做越来，没有我有？\n",
      "\n",
      "-\n",
      "Input sentence: 人生如戏，全靠演技。小受吓坏了。\n",
      "Decoded sentence: 人生如戏，全靠演技。\n",
      "\n",
      "-\n",
      "Input sentence: 为什么这世上会有人以刁难他人为乐呢？\n",
      "Decoded sentence: 是不能正的地举。\n",
      "\n",
      "-\n",
      "Input sentence: 算了算了，我看出来了，你们都想看男人！上张美男图。\n",
      "Decoded sentence: 多放点男模的照片看看\n",
      "\n",
      "-\n",
      "Input sentence: 偷拍时被喵星人发现了。！\\3/\n",
      "Decoded sentence: 以前在物流公司，举行过驾驶员滚轮胎比赛，拍了照片，挺有意思\n",
      "\n",
      "-\n",
      "Input sentence: 看看你的名字在古代是什么职业，太让人崩溃了。\n",
      "Decoded sentence: 人生如戏，全靠演技。\n",
      "\n",
      "-\n",
      "Input sentence: 居然把WindowsPhone8写成Win8了……要严谨啊\n",
      "Decoded sentence: 不知道为什么有这样？\n",
      "\n",
      "-\n",
      "Input sentence: 百合真香啊！送给回家路上的人们明天后天就不行了！\n",
      "Decoded sentence: 还是个可爱的小胖纸\n",
      "\n",
      "-\n",
      "Input sentence: 刘翔预算摔倒！无缘半决赛！主持人哭了。\n",
      "Decoded sentence: 是的是一个小总要什么的时候了？\n",
      "\n",
      "-\n",
      "Input sentence: 谁身边没几个能办大事的朋友？\n",
      "Decoded sentence: 好想吃，可是我家里没有呢？\n",
      "\n",
      "-\n",
      "Input sentence: 一位妈妈将她四岁儿子的涂鸦做成了真实的毛绒玩具\n",
      "Decoded sentence: 喜欢第三张图。明亮的时候力点了\n",
      "\n",
      "-\n",
      "Input sentence: 竟然下雪了，不喜欢冬天，天气何时才变暖啊。\n",
      "Decoded sentence: 当然做黑了：我可以慢提，，后是应该健康、\n",
      "\n",
      "-\n",
      "Input sentence: 困扰我多年的问题终于得解了。\n",
      "Decoded sentence: 哎哟喂。立马石化了。\n",
      "\n",
      "-\n",
      "Input sentence: 大学生一定要看的一分钟，它能让你奋斗一辈子alink\n",
      "Decoded sentence: 这是哪里面得马吗？我们呢？\n",
      "\n",
      "-\n",
      "Input sentence: #米吧秀图#月底了，敢说出你手机还有多少流量么？\n",
      "Decoded sentence: 这个图片是谁照的啊，真是绝啊\n",
      "\n",
      "-\n",
      "Input sentence: 我擦！这么多人来看我！\n",
      "Decoded sentence: 是的样的！应该是那么的哇？\n",
      "\n",
      "-\n",
      "Input sentence: 各位都到了吗？深圳一会见哈：））\n",
      "Decoded sentence: 哈哈哈哈哈哈哈。做了三年纪律委员没做过波😏😏\n",
      "\n",
      "-\n",
      "Input sentence: 如果有人煮这个汤给你喝，你会感动吗？\n",
      "Decoded sentence: 是不能拍的秘密，就是个告诉你都不认识\n",
      "\n",
      "-\n",
      "Input sentence: 上传了23张照片到“微相册”。alink\n",
      "Decoded sentence: 好想看，连上鸡的都有点油，这种，让我们然\n",
      "\n",
      "-\n",
      "Input sentence: 看见有黑点的要注意休息了。\n",
      "Decoded sentence: 大家一块预测都说有发意甲车的明年的课练同个\n",
      "\n",
      "-\n",
      "Input sentence: 若慈善不透明，则捐款无意义。\n",
      "Decoded sentence: 中国黄金年代的feel。真的不一样。\n",
      "\n",
      "-\n",
      "Input sentence: 成熟的最大好处是：以前得不到的，现在不想要了。\n",
      "Decoded sentence: 真想看下他的正面是怎么样的\n",
      "\n",
      "-\n",
      "Input sentence: 编织和仿旧的风格，看起来很有质感\n",
      "Decoded sentence: 如果这个星球都认识\n",
      "\n",
      "-\n",
      "Input sentence: 听到这周要连上7天的消息后，我突然意识到。\n",
      "Decoded sentence: 哎哟喂。立马石化了。\n",
      "\n",
      "-\n",
      "Input sentence: 慢镜头回放狗狗看到食物的可爱反应！alink\n",
      "Decoded sentence: 好想吃一起可以喝到ware手机里的学e了。\n",
      "\n",
      "-\n",
      "Input sentence: 个人经验，普通话过于标准的人一般干不了什么大事。\n",
      "Decoded sentence: 没事，这就是荷兰，换个主帅，2014再看郁金香绽放。\n",
      "\n",
      "-\n",
      "Input sentence: 喵星人个个都是武林高手！\n",
      "Decoded sentence: 哈哈，这是真的爱，我帅啊！\n",
      "\n",
      "-\n",
      "Input sentence: 2012地球不会毁灭，因为多啦A梦告诉我们他是从2070年来的。\n",
      "Decoded sentence: 要是纹身就纹个哆啦A梦\n",
      "\n",
      "-\n",
      "Input sentence: #虎扑话题#谁是你心中外貌最帅气的教练？alink\n",
      "Decoded sentence: 这是个人，负爱走出人，或许是这人。认这身心的很偶\n",
      "\n",
      "-\n",
      "Input sentence: 苹果不要只削一半好不好！你们考虑过苹果的感受嘛！\n",
      "Decoded sentence: 的确伤美。手机换的女儿。\n",
      "\n",
      "-\n",
      "Input sentence: 转眼，他走了16年。如果，他还活着……\n",
      "Decoded sentence: 这样的话我们都可以问，唯独你不能问！\n",
      "\n",
      "-\n",
      "Input sentence: alink讲好公子和坏公子的文章，不错。凤凰网。\n",
      "Decoded sentence: 多这就对，对我起来吃草！\n",
      "\n",
      "-\n",
      "Input sentence: 杨怡真的好像马啊！好想喂她吃草！\n",
      "Decoded sentence: 只要肯花心思什么复杂的都觉得吃\n",
      "\n",
      "-\n",
      "Input sentence: 【治愈系图片】又到了毕业季——这么有爱的毕业照你们见过吗？\n",
      "Decoded sentence: 我也太有意思了一句话的。马子这么多呢，这句爸我们老一下\n",
      "\n",
      "-\n",
      "Input sentence: 科普贴：《大林和小林》剧照\n",
      "Decoded sentence: 看着好吓人啊！有没有人同感？\n",
      "\n",
      "-\n",
      "Input sentence: 另外一个坏消息，巴恩斯续约快船，孩子们，咱又没戏了。\n",
      "Decoded sentence: 没有所谓。被过你也做肉西性。\n",
      "\n",
      "-\n",
      "Input sentence: 过去我们再也回不去了，\n",
      "Decoded sentence: 我也在天河城附近哈哈不知道有木有机会看到你\n",
      "\n",
      "-\n",
      "Input sentence: 一群垃圾的制造者，当然也包括我自己。\n",
      "Decoded sentence: 应该是一群资源的遗弃者\n",
      "\n",
      "-\n",
      "Input sentence: 走近科学：神农架野香蕉。国外又称死人手指\n",
      "Decoded sentence: 能吃吗…………好想吐……\n",
      "\n",
      "-\n",
      "Input sentence: 不要恋家！又要开始训练啦！不想回家，一点也不想\n",
      "Decoded sentence: 不是用了，国家的试院……\n",
      "\n",
      "-\n",
      "Input sentence: #郑在提醒吃早餐#没忘了吃早饭吧？餐桌上这段文字出自哪部作品？\n",
      "Decoded sentence: 大叔的早餐好有营养！\n",
      "\n",
      "-\n",
      "Input sentence: 男子因女儿哭闹一脚将其踢死alink\n",
      "Decoded sentence: 小孩哭闹的确很烦人只能说这位父亲点背给踢死了\n",
      "\n",
      "-\n",
      "Input sentence: 当你觉得生活没意思的时候。(转)\n",
      "Decoded sentence: 会不会是“安神”吧？\n",
      "\n",
      "-\n",
      "Input sentence: 百度官方微博即将上线，敬请期待！\n",
      "Decoded sentence: 中国搜索引擎的大佬也来微博了。\n",
      "\n",
      "-\n",
      "Input sentence: 长春柏翠园小区绿化植物之一：伞形紫花，灌木，科属种？\n",
      "Decoded sentence: 你意思是一个图片是在望年识过可能的回事的地不一。\n",
      "\n",
      "-\n",
      "Input sentence: #NBA总决赛#热火击败马刺，卫冕成功。\n",
      "Decoded sentence: 老兵永远不死。虐心的第六场！\n",
      "\n",
      "-\n",
      "Input sentence: 据说，强迫症的童鞋看完之后很难受。「转」\n",
      "Decoded sentence: 人民战士。做时自几个那些看\n",
      "\n",
      "-\n",
      "Input sentence: 谁是谁的如花美眷，谁错过了谁的似水流年。\n",
      "Decoded sentence: 你好，渺小的＂鱼＂\n",
      "\n",
      "-\n",
      "Input sentence: 人的眼睛有5.76亿像素但却终究看不懂人心。「转」\n",
      "Decoded sentence: 这个都是哪里的吧？好想眼？\n",
      "\n",
      "-\n",
      "Input sentence: 好像反动派们都不在这里？\n",
      "Decoded sentence: 你就是我喜欢不上别人的理由\n",
      "\n",
      "-\n",
      "Input sentence: 农夫山泉右边是假的，大家转发告诉身边的朋友。\n",
      "Decoded sentence: 早被农夫山泉辟谣了，两个都是真的。\n",
      "\n",
      "-\n",
      "Input sentence: 不去刻意造作自己的心自然自在随性而天真\n",
      "Decoded sentence: 是的世上的天气很好有那么多\n",
      "\n",
      "-\n",
      "Input sentence: 巴萨官方宣布阿根廷人马蒂诺出任球队新帅！\n",
      "Decoded sentence: 我有我也不小朋友的一片小我里哈哈\n",
      "\n",
      "-\n",
      "Input sentence: 多关注新电影＜致命闪玩＞💓💓\n",
      "Decoded sentence: 个月3的那是个爱人一下\n",
      "\n",
      "-\n",
      "Input sentence: 三峡大坝质量如何？诡异。分享图片\n",
      "Decoded sentence: 是这样的，能不能做一片好的。\n",
      "\n",
      "-\n",
      "Input sentence: 老师改到这份答题卡的时候，跳起来了。\n",
      "Decoded sentence: 哈哈。我也不敢说去查\n",
      "\n",
      "-\n",
      "Input sentence: 世上最幸福的事：坏脾气的她遇到好脾气的他，他却爱上了她。\n",
      "Decoded sentence: 这是个人，现在我不对面、\n",
      "\n",
      "-\n",
      "Input sentence: 据说，这是布拉德皮特写给妻子的信——《爱的秘密》。\n",
      "Decoded sentence: 说是消防车进不去？\n",
      "\n",
      "-\n",
      "Input sentence: 在飞往尼泊尔的机程一口气看完。\n",
      "Decoded sentence: 我觉得上的才是最棒的\n",
      "\n",
      "-\n",
      "Input sentence: 据说是二年级的数学题你会吗？「转」\n",
      "Decoded sentence: 说到底，李大霄人品没有底\n",
      "\n",
      "-\n",
      "Input sentence: 法网女单决赛李娜VS斯齐亚沃尼CCTV5视频直播：alink\n",
      "Decoded sentence: 好可爱啊，以后也找吃那么生的\n",
      "\n",
      "-\n",
      "Input sentence: 乞丐虽穷但有人格，猪狗不如畜生是也！\n",
      "Decoded sentence: 这又是那个狗娘养的，骂已出口，对不起这位畜生的母亲了。\n",
      "\n",
      "-\n",
      "Input sentence: 各种失误，你们感受一下。\n",
      "Decoded sentence: 神龙，侠a小r，，要不给我次个表情。\n",
      "\n",
      "-\n",
      "Input sentence: 吃泡面的最高境界！你有木有！\n",
      "Decoded sentence: 多放安的日！想想要一定\n",
      "\n",
      "-\n",
      "Input sentence: 诺基亚Lumia710组成的多米诺，想不想推倒TA？\n",
      "Decoded sentence: 想推倒TA，再送我三个颜色的就够了！\n",
      "\n",
      "-\n",
      "Input sentence: 请各位朋友不要把我列入自媒体.谢谢.\n",
      "Decoded sentence: 是男的人，我不像生中，哥\n",
      "\n",
      "-\n",
      "Input sentence: 叶秉桓刚刚示范了一下「转音转到吐」。声线挺棒。\n",
      "Decoded sentence: 没有GD，，故地d要不块g吧，\n",
      "\n",
      "-\n",
      "Input sentence: 童鞋们，咱们一起过六一喽😁😁😁\n",
      "Decoded sentence: 我也在天河城附近哈哈哈最知道有些人陪不开心在看之后。\n",
      "\n",
      "-\n",
      "Input sentence: 好怀恋的宿舍生活，有同感的转吧！（via陈骏基）\n",
      "Decoded sentence: 真想看一半是美好的，它，我们1013报一样生。\n",
      "\n",
      "-\n",
      "Input sentence: 据央视刚刚报道，神八飞船发射时间确定为今日5时58分10秒。\n",
      "Decoded sentence: 神八威武，坐等发射\n",
      "\n",
      "-\n",
      "Input sentence: 欠自己的旅行，有一天，一定要还给自己！\n",
      "Decoded sentence: 这个说我的日了生活\n",
      "\n",
      "-\n",
      "Input sentence: 有多少人，还记得当年学校的校训的，请举手！\n",
      "Decoded sentence: 得到学····会会会被微笑。\n",
      "\n",
      "-\n",
      "Input sentence: 打车经过伦敦塔，皇室的监禁地和处死地。#Weico拼图#\n",
      "Decoded sentence: 要求是新个哆啦，试过。要一个给视见，\n",
      "\n",
      "-\n",
      "Input sentence: 哎呀不关我事。是系统自动发送的……我是小倩alink\n",
      "Decoded sentence: 大炮，有点职业道就看我。\n",
      "\n",
      "-\n",
      "Input sentence: 别以为我脸大，就可以随便捏我脸！\n",
      "Decoded sentence: 成功者总是不约而同的满足时代的需要\n",
      "\n",
      "-\n",
      "Input sentence: 很开心跟你们聊天，晚安\n",
      "Decoded sentence: 你是睿智的成熟女人。爱你。晚安\n",
      "\n",
      "-\n",
      "Input sentence: 记住每一个善待你的人，因为他们本可以不这么做。\n",
      "Decoded sentence: 小时候，在晒谷场放映投影电影，最深刻的是恐怖片《画皮》。\n",
      "\n",
      "-\n",
      "Input sentence: 仙剑小说2卖得不错，这是京东的排行榜，销售排行榜第一。\n",
      "Decoded sentence: 人生就像摸头，你永远不知道下isind\n",
      "\n",
      "-\n",
      "Input sentence: 为了iPhone打字大赛，我的手做了个小手术。\n",
      "Decoded sentence: 一点浩然气，千里快哉风\n",
      "\n",
      "-\n",
      "Input sentence: 一定要买这张CD，#李霄云#的《你看到的我是蓝色的》\n",
      "Decoded sentence: 真的好美好的一片！小说\n",
      "\n",
      "-\n",
      "Input sentence: 好声音，一在室外唱现场，怎么就那么大差距啊。\n",
      "Decoded sentence: 这个喜欢在心里的人，好像你自己看一下？\n",
      "\n",
      "-\n",
      "Input sentence: 未来十天在波兰，手机不通，有事麻烦私信或邮件吧。\n",
      "Decoded sentence: 转眼两年前我也在波兰\n",
      "\n",
      "-\n",
      "Input sentence: 我这种壮汉，一穿细高跟鞋，就好像在欺凌强暴它。\n",
      "Decoded sentence: 不会不是不想下那么多中国吗？\n",
      "\n",
      "-\n",
      "Input sentence: 他们也曾是冠军，但。请看图（转）\n",
      "Decoded sentence: 看了我💛手机会呢？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 工作圆满完成，今天要回台湾啰！北京•再见。\n",
      "Decoded sentence: 老罗今晚去李志的演出么，有登台献唱的计划么\n",
      "\n",
      "-\n",
      "Input sentence: 准备夜战，辞职了，失业了，求保养\n",
      "Decoded sentence: 哇，我也看过，也会是爱人，我能失去老然和追坐一个。\n",
      "\n",
      "-\n",
      "Input sentence: 把答案，变成你喜欢的样子。（via伟大的安妮）\n",
      "Decoded sentence: 是否小时候也就像单位，我外人其他不想已经失了。\n",
      "\n",
      "-\n",
      "Input sentence: 为什么要关灯一小时呢？剩下57分钟做什么？\n",
      "Decoded sentence: 当然是黑崎一护家的魂，然后是后后一个城后老贝才是多。\n",
      "\n",
      "-\n",
      "Input sentence: 一篇日记，多么自律的小童鞋，让人内牛满面呀……\n",
      "Decoded sentence: 这人说得这么做了，以后会不对啊\n",
      "\n",
      "-\n",
      "Input sentence: 今天两家中国公司IPO取消。如果我没弄错的话。\n",
      "Decoded sentence: 这是要心情。我也不住到它班，这个晚\n",
      "\n",
      "-\n",
      "Input sentence: 近年最喜欢的两句诗：他们摧残花朵，以为能阻止春天的到来……\n",
      "Decoded sentence: 哈哈，这是怎么游的豆腐渣工程啊\n",
      "\n",
      "-\n",
      "Input sentence: 狗宝宝的一天，太催泪了！慎入……「转」\n",
      "Decoded sentence: 不能太不用了一只是老王说这样说\n",
      "\n",
      "-\n",
      "Input sentence: #Lens夜话#你平常喜欢收集/珍藏________________\n",
      "Decoded sentence: 我也不相信作家会不过去的主要还给我吧\n",
      "\n",
      "-\n",
      "Input sentence: 难得的早饭就来这一口。\n",
      "Decoded sentence: 老郑发这个有意义吗？一只要打天比！\n",
      "\n",
      "-\n",
      "Input sentence: 朋友们早上好，今天的空气质量。\n",
      "Decoded sentence: 真心的。现实都是有人吗？这样又有一个人。\n",
      "\n",
      "-\n",
      "Input sentence: 主人，救救我，帮我翻下身。\n",
      "Decoded sentence: 我去天了一个小认。！\n",
      "\n",
      "-\n",
      "Input sentence: 我们来#随手拍#标语中国吧\n",
      "Decoded sentence: 是不是在我这个我，我怕失去了也强大的恐怖。\n",
      "\n",
      "-\n",
      "Input sentence: 他们真的准备去保护地球么？那个谁，要不要那么露富啊？\n",
      "Decoded sentence: 小时候，在晒谷场放映投影电影，最深刻的是恐怖片《画皮》。\n",
      "\n",
      "-\n",
      "Input sentence: 买蛋糕，就是要这样买！老板一个8寸蛋糕！混合口味！\n",
      "Decoded sentence: 你要拿我的日本吗，怎么都觉得那么早呢\n",
      "\n",
      "-\n",
      "Input sentence: 珍惜那个每天跟你说晚安的人。\n",
      "Decoded sentence: 话说我好想忘记了蜘蛛精最后怎么样了\n",
      "\n",
      "-\n",
      "Input sentence: 每个人对你好，都不是义务的。所以，要珍惜每一个对你好的人。\n",
      "Decoded sentence: 真是，你回来就不。\n",
      "\n",
      "-\n",
      "Input sentence: 四五个小时长途，实在无聊。谁来聊五毛钱的？\n",
      "Decoded sentence: 没有人陪我的鲸鱼，都是对她\n",
      "\n",
      "-\n",
      "Input sentence: 姥姥，就差你了……#大尸凶的漫画#\n",
      "Decoded sentence: 我家狗上回来了，可以让我认认真的，看，这是神一效果\n",
      "\n",
      "-\n",
      "Input sentence: 辣么辣么热，你的城市此时此刻多少度？你在干什么？\n",
      "Decoded sentence: 老大家刚刚当日的东西还以这小用货\n",
      "\n",
      "-\n",
      "Input sentence: Imissyou，butIwon'tbotheryou.我想你，但我不会打扰你。\n",
      "Decoded sentence: 哈哈哈哈Smartisthenewsexy！\n",
      "\n",
      "-\n",
      "Input sentence: 如果你知道这个游戏的话真是恭喜你了，正式加入老龄组。「转」\n",
      "Decoded sentence: 如果你学了，我还在我我还是对我。\n",
      "\n",
      "-\n",
      "Input sentence: 好像微博出了点问题，试试看发带图的微博，你们能看到吗？\n",
      "Decoded sentence: 你就是不是在我家想起去晚在都特时还有多强的。\n",
      "\n",
      "-\n",
      "Input sentence: 熊市啊，中午来碗菜泡饭算了。\n",
      "Decoded sentence: 真不是这样走国人。第一周六这么多，在中国众\n",
      "\n",
      "-\n",
      "Input sentence: 一大早，60后和80后又吵上了，都嚷嚷着要自由\n",
      "Decoded sentence: 感觉笔者是五月天的粉\n",
      "\n",
      "-\n",
      "Input sentence: #最内涵#神奇的中文，你们感受一下\n",
      "Decoded sentence: 像潘小时候熟悉的风物吧。\n",
      "\n",
      "-\n",
      "Input sentence: 我和我的小伙伴都惊呆了\n",
      "Decoded sentence: 不要说我也想要在老！\n",
      "\n",
      "-\n",
      "Input sentence: 石家庄街边某小饭馆，那威开怀畅饮，祝我生日快乐。\n",
      "Decoded sentence: 看子都别人很可爱。\n",
      "\n",
      "-\n",
      "Input sentence: 第一次坐高铁，感觉不错…，晚上德州三千人在等我\n",
      "Decoded sentence: 想在woftorlovaco.\n",
      "\n",
      "-\n",
      "Input sentence: 想和闺蜜一起拍一组这样的婚纱照吗？\n",
      "Decoded sentence: 一点浩然气，千里快哉风\n",
      "\n",
      "-\n",
      "Input sentence: 世界末日，未是末日；末日未日，方是末日——禅师赠青年偈子\n",
      "Decoded sentence: 一点浩然气，千里快哉风\n",
      "\n",
      "-\n",
      "Input sentence: 年末了，我很缺钱。和我一样的童鞋默默转发。\n",
      "Decoded sentence: 有人家长得一玩都没有啦！\n",
      "\n",
      "-\n",
      "Input sentence: 微信正在密谋转移淘宝天猫优质商家？\n",
      "Decoded sentence: 一个人活在世上又怎么做到这些看似简单的几个词呢\n",
      "\n",
      "-\n",
      "Input sentence: 图片解禁了，中国人的心何时解禁！\n",
      "Decoded sentence: 新年快乐！求祝老师！\n",
      "\n",
      "-\n",
      "Input sentence: 今天考政治一激动，差点把先锋队写成先锋盾……\n",
      "Decoded sentence: 我也不喜欢的，我这个吊。\n",
      "\n",
      "-\n",
      "Input sentence: 监控录像看到的一群非法入室者。喵星人太牛了！（转）OMG。\n",
      "Decoded sentence: 有时候幸福其实很简单\n",
      "\n",
      "-\n",
      "Input sentence: #没品图#太黑了……谁没获得过欧冠冠军？\n",
      "Decoded sentence: 为什么不要这边对称的？\n",
      "\n",
      "-\n",
      "Input sentence: 看完了！准备买个陀螺去！\n",
      "Decoded sentence: 是男的体温不怎么正常哦。\n",
      "\n",
      "-\n",
      "Input sentence: 日和漫画【全集】终于可以一次笑个够啦。。alink\n",
      "Decoded sentence: 好有爱！\n",
      "\n",
      "-\n",
      "Input sentence: 当时霍比特人的宣传海报是画上去的\n",
      "Decoded sentence: 我擦。毫无违和感。\n",
      "\n",
      "-\n",
      "Input sentence: ❤海浪的诱惑，是它征服了你，还是你被它征服了。\n",
      "Decoded sentence: 很有意义，一点都不无聊。说无聊的，是他还不明白喝水的重要性\n",
      "\n",
      "-\n",
      "Input sentence: 从大陆到香港工作的人你伤不起。\n",
      "Decoded sentence: 老郑，你手酸不酸啊。以后限2个小时\n",
      "\n",
      "-\n",
      "Input sentence: 再不开心，也要试着笑。至少你努力笑一笑就不会感到那么难过。\n",
      "Decoded sentence: 就是嘛，胜不骄败不馁！才是奥林匹克精神的宗旨\n",
      "\n",
      "-\n",
      "Input sentence: 亲们……警用新装备哦，每分钟6000发哦！颤抖吧！凡人们！\n",
      "Decoded sentence: 就是，我们打连生命中好离。\n",
      "\n",
      "-\n",
      "Input sentence: 这是北京还是北极啊？耳朵都吹没了\n",
      "Decoded sentence: 大海就不能遇到裂哥\n",
      "\n",
      "-\n",
      "Input sentence: 1967年大陆工业地图，Source：CIA点击”查看大图“\n",
      "Decoded sentence: 我喜欢的人感动，这句，你叫不想了那什看。\n",
      "\n",
      "-\n",
      "Input sentence: 比首都机场更锻炼身体的是昆明新机场。\n",
      "Decoded sentence: 我擦，小无聊和小德超的还。\n",
      "\n",
      "-\n",
      "Input sentence: 请记得感恩，因为没有人天生就应该对你好。\n",
      "Decoded sentence: 是的小的车是世界的有什么啊？\n",
      "\n",
      "-\n",
      "Input sentence: 看来想坑喵星人是行不通了→＿→「转」\n",
      "Decoded sentence: 很喜欢您写的词，呵呵第一次这么靠前，吼吼\n",
      "\n",
      "-\n",
      "Input sentence: 好久没有感受到落汤鸡的销魂体验。\n",
      "Decoded sentence: 我是云，闪电是我的朋友！\n",
      "\n",
      "-\n",
      "Input sentence: 我准备做家具生意，这款产品谁看上了可以预定\n",
      "Decoded sentence: 是不是在我老面。我就是不过。\n",
      "\n",
      "-\n",
      "Input sentence: 义乌的海鲜大排档，和老同学喝酒，听歌。致我们逝去的青春！\n",
      "Decoded sentence: 你们居然跑去唱歌了！\n",
      "\n",
      "-\n",
      "Input sentence: #随手拍#五一假期，难得有机会接触大自然！带侄女玩儿一玩儿！\n",
      "Decoded sentence: 这个广告词好！永久牌自行车\n",
      "\n",
      "-\n",
      "Input sentence: 刚站在窗前，我—眼就瞧见了那个，可心的小妞。\n",
      "Decoded sentence: 人生苦短，只争朝夕！\n",
      "\n",
      "-\n",
      "Input sentence: #旅途美食#大家来说说，在你的家乡，这个水果叫什么？\n",
      "Decoded sentence: 就算做了，梦到的人真可不能，，但是那是棉的款是\n",
      "\n",
      "-\n",
      "Input sentence: 五种谜一样的生物。（via犯贱志）\n",
      "Decoded sentence: 谁在这里面。法国能有清思？\n",
      "\n",
      "-\n",
      "Input sentence: 试了下点点，发现不会用。\n",
      "Decoded sentence: 好吧……P的太有水了……\n",
      "\n",
      "-\n",
      "Input sentence: 每个人出生的时候都是原创，可悲的是很多人渐渐都成了盗版。\n",
      "Decoded sentence: 真的很薄。概觉的太白了！\n",
      "\n",
      "-\n",
      "Input sentence: 再两天就是新年了，又可以有借口大吃大喝:)\n",
      "Decoded sentence: 当然愿意了！来一点我觉得有点做饭！\n",
      "\n",
      "-\n",
      "Input sentence: 解决早晨赖床的概念应用！\n",
      "Decoded sentence: 你好！渺小时候啊？\n",
      "\n",
      "-\n",
      "Input sentence: 从听不腻到听不得，也就十年。唉。深表遗憾。\n",
      "Decoded sentence: 当新鲜的空气成为奢侈品之后。\n",
      "\n",
      "-\n",
      "Input sentence: #每日一神句#我说“千锤百炼出深山，”你接_________________？\n",
      "Decoded sentence: 小时候，在晒谷场放映投影电影，最深刻的是恐怖片《画皮》。\n",
      "\n",
      "-\n",
      "Input sentence: 爱琴海的日落，另一种颜色的浪漫。\n",
      "Decoded sentence: 我家狗也只要因为的地里有这样美女人将将？\n",
      "\n",
      "-\n",
      "Input sentence: 在国际睡眠日早早起床干活儿，其实本质上就是反人类和反社会！\n",
      "Decoded sentence: 这是视力表！请求搞张清晰一点的！\n",
      "\n",
      "-\n",
      "Input sentence: 一张狠图告诉你什么叫做：坚持就是胜利！(转)\n",
      "Decoded sentence: 是啊！你在哪里都得？！\n",
      "\n",
      "-\n",
      "Input sentence: 分享了一篇文章：《冬日》\n",
      "Decoded sentence: 人生的需要形象的小M练是这样的\n",
      "\n",
      "-\n",
      "Input sentence: 这是一封简单的小情书。\n",
      "Decoded sentence: 感觉不错··但是那放毛巾的架子···\n",
      "\n",
      "-\n",
      "Input sentence: 【如何种柠檬】气味芬芳，自己种一盆吧。（几分钟网）#小知识#\n",
      "Decoded sentence: 没有GDP，也没有现在詹姆斯！时代的终结，永远不变的是怀念！\n",
      "\n",
      "-\n",
      "Input sentence: 保护她不是你的任务，而是我的。------《碟中谍4》\n",
      "Decoded sentence: 是啊！你那里也会这样吗？\n",
      "\n",
      "-\n",
      "Input sentence: 没看过含羞草害羞的童鞋点击看看.\n",
      "Decoded sentence: 没看，就是这样的变成。\n",
      "\n",
      "-\n",
      "Input sentence: 生活一定要五颜六色，但绝不能乱七八糟。早安。\n",
      "Decoded sentence: 不是吧，太用了吧？好了今天今天没来磨磨看看着---痛\n",
      "\n",
      "-\n",
      "Input sentence: 【微漫画】这是最幸福的时刻！没有之一！有同感的请举手！\n",
      "Decoded sentence: 这个不是在我家学里下\n",
      "\n",
      "-\n",
      "Input sentence: 开车到布拉格庄园来玩，让姑爷给我照一张照片，感觉不错。\n",
      "Decoded sentence: 你好，我喜欢你没事，你以后我也在的我去在去老刘。\n",
      "\n",
      "-\n",
      "Input sentence: 禁止买菜刀，是为了保障大会进行重大变革。--院里传出来的消息。\n",
      "Decoded sentence: 的确是美女版。哈哈哈哈\n",
      "\n",
      "-\n",
      "Input sentence: 主人，你整死我了，喵。「图转」\n",
      "Decoded sentence: 你还是我喜欢不上别人的你点\n",
      "\n",
      "-\n",
      "Input sentence: 实践再次证明，猫比猫，气死银。\n",
      "Decoded sentence: ，文地方便说以说了一个什么幸福，吃\n",
      "\n",
      "-\n",
      "Input sentence: 你幸福吗？此时此刻，一个人喝茶，也没什么好想的，算幸福吗？\n",
      "Decoded sentence: 我认识，家家喝试啊…\n",
      "\n",
      "-\n",
      "Input sentence: 豆瓣首页的新变化alink\n",
      "Decoded sentence: 从来木有点，先生把打多年前。在安的生！\n",
      "\n",
      "-\n",
      "Input sentence: 到达宁波了连续三个客场真累呀！\n",
      "Decoded sentence: 好可爱啊，以后也给我的宝宝买。\n",
      "\n",
      "-\n",
      "Input sentence: 办公室必备的喝可乐利器，夏天必备啊。\n",
      "Decoded sentence: 老爷子老爸我真好。\n",
      "\n",
      "-\n",
      "Input sentence: #chunwan#基佬完了就是伪娘么？\n",
      "Decoded sentence: 想推倒TA，再送我三个颜色的就够了！\n",
      "\n",
      "-\n",
      "Input sentence: #2012高考#【吐槽】今天上午某考场外一景。\n",
      "Decoded sentence: 是的，我想在我生不能在明我还在长大外这样。\n",
      "\n",
      "-\n",
      "Input sentence: 开学第一天，运营商校园广告就大PK上了，太有爱了！\n",
      "Decoded sentence: 小孩子看你，什么看主去，我手机又不想你那个老婆的玩点\n",
      "\n",
      "-\n",
      "Input sentence: 人生中10件无能为力的事。\n",
      "Decoded sentence: 喜欢心理。西！北京下来了这就更了\n",
      "\n",
      "-\n",
      "Input sentence: 从前有个乐队主唱他的裤子一直往下滑。\n",
      "Decoded sentence: 很喜欢这句话。哈哈。\n",
      "\n",
      "-\n",
      "Input sentence: 喘口气吧，还在工作台前忙碌的人们！——terra旅游创意广告\n",
      "Decoded sentence: 人生如戏，全靠演技。这是很意思\n",
      "\n",
      "-\n",
      "Input sentence: Facebook今晚上市，如果是你，你会买进Facebook股票么？为什么？\n",
      "Decoded sentence: 有人活着只是因为未来还伴有希望\n",
      "\n",
      "-\n",
      "Input sentence: 开始计算选票了，佛罗里达很关键，现在奥巴马领先51％\n",
      "Decoded sentence: 早安。好好。被儿。\n",
      "\n",
      "-\n",
      "Input sentence: 姑娘，你真是条汉子。「转」\n",
      "Decoded sentence: 你不曾给我一次回眸，我却始终在对你自己看到你。\n",
      "\n",
      "-\n",
      "Input sentence: 姗姗来迟的亨利·米勒《南回归线》《北回归线》\n",
      "Decoded sentence: 好可爱啊，是如现在开始原来说该这大了干费有多也吃了\n",
      "\n",
      "-\n",
      "Input sentence: 一句话不咯。高跟鞋扛桶装水的女汉子，霸气四溢(转)\n",
      "Decoded sentence: 其实你们不懂女汉子内心的苦啊没有男人只能自己扛\n",
      "\n",
      "-\n",
      "Input sentence: 2011年中国最新恐怖巨作，超越日式泰式恐怖。\n",
      "Decoded sentence: 真玩，这样想什么总会有比你呢？\n",
      "\n",
      "-\n",
      "Input sentence: 不可能每天都会有高潮，这就是人生。\n",
      "Decoded sentence: 哇塞。好亮的眼睛！\n",
      "\n",
      "-\n",
      "Input sentence: 【武汉公交】永恒的神话，中国F1的希望……（转帖）\n",
      "Decoded sentence: 有一种奇怪的感觉油然而生。\n",
      "\n",
      "-\n",
      "Input sentence: 习总受到梅总的热情接待。（RIA-Novosti）\n",
      "Decoded sentence: 哈哈，最后一张太2了！\n",
      "\n",
      "-\n",
      "Input sentence: 考试学生类型划分图，你是哪一型？（源自网络）\n",
      "Decoded sentence: 你们居然跑去唱歌了！\n",
      "\n",
      "-\n",
      "Input sentence: 早上好，你跟别人打过赌吗\n",
      "Decoded sentence: 我想说，这是要肯定啊\n",
      "\n",
      "-\n",
      "Input sentence: 还能接着坚持看下去的凯蜜，都是真爱。\n",
      "Decoded sentence: 很喜欢这样风格的，漂亮很希望有无能给我们。\n",
      "\n",
      "-\n",
      "Input sentence: 当你不再期待什么东西的时候，你会得到一切。\n",
      "Decoded sentence: 还是wadi么好看看了吧.音们的是哪儿也不好女\n",
      "\n",
      "-\n",
      "Input sentence: 【《环太平洋》各国机甲】你觉得哪个国家的机甲最帅！？「转」\n",
      "Decoded sentence: 当官做到这样，走到哪儿都需要一副厚脸皮才得行哟！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_sample = len(encoder_input_data)\n",
    "for seq_index in range(num_sample-100, num_sample):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = open(\"LSTMresult.txt\", \"w\",encoding=\"utf8\")\n",
    "for seq_index in range(len(encoder_input_data)):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    fo.write(decoded_sentence)\n",
    "    \n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = open(\"LSTManswer.txt\", \"w\",encoding=\"utf8\")\n",
    "for seq_index in range(len(encoder_input_data)):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "    fo.write(decoded_sentence)\n",
    "    \n",
    "fo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
